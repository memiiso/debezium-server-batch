# ==================== SOURCE DB == MYSQL ====================

debezium.source.connector.class=io.debezium.connector.mysql.MySqlConnector
debezium.source.offset.storage=org.apache.kafka.connect.storage.FileOffsetBackingStore
debezium.source.offset.storage.file.filename=data/offsets.dat
debezium.source.offset.flush.interval.ms=60000

debezium.source.database.hostname=127.0.0.1
debezium.source.database.port=3306
debezium.source.database.user=user
debezium.source.database.password=xyz
debezium.source.database.dbname=mydb
debezium.source.database.ssl.mode=required

# false = ("op": "r") AND true = ("op": "c")
debezium.source.snapshot.events.as.inserts=false

# when decimal.handling.mode configuration property is set to double, then the connector will represent
# all DECIMAL and NUMERIC values as Java double values and encodes them as follows:
debezium.source.decimal.handling.mode=double

# ==================== SINK ====================

debezium.sink.type=batch
debezium.sink.batch.writer=spark
debezium.sink.sparkbatch.bucket-name=s3a://my-Bucket
debezium.sink.sparkbatch.save-format=parquet
debezium.sink.batch.objectkey-prefix=rawdata/cdc/
debezium.sink.batch.objectkey-partition=true
# max batch size per target
debezium.sink.batch.row-limit=10000
# max time limit per batch. 1800 second = 30 min
debezium.sink.batch.time-limit=1800

# ==================== SINK = CACHE ====================
# false uses infinispan local cache
debezium.sink.batch.cache.simple=false
debezium.sink.batch.cache.location=data/cache
debezium.sink.batch.cache.memory-maxcount=128
debezium.sink.batch.cache.purge-on-startup=false

# ==================== SINK == SPARK CONF ====================
debezium.sink.sparkbatch.spark.ui.enabled=false

# ==================== FORMAT ====================

# enable schema
debezium.format.value.schemas.enable=true

# ==================== TRANSFORM ====================
# debezium unwrap message
debezium.transforms=unwrap
debezium.transforms.unwrap.type=io.debezium.transforms.ExtractNewRecordState
debezium.transforms.unwrap.add.fields=op,table,source.ts_ms
debezium.transforms.unwrap.add.headers=db
# rewrite => The value field directly contains the key/value pairs that were in the before field. The SMT adds __deleted and sets it to true, for example:
debezium.transforms.unwrap.delete.handling.mode=rewrite
debezium.transforms.unwrap.drop.tombstones=false


# ==================== Misc ====================

debezium.source.database.history.kafka.bootstrap.servers=kafka:9092
debezium.source.database.history.kafka.topic=dbhistory.fullfillment
debezium.source.include.schema.changes=false
debezium.source.database.history=io.debezium.relational.history.FileDatabaseHistory
debezium.source.database.history.file.filename=data/dbhistory.txt

quarkus.http.port=8014

# ==================== LOG LEVELS ====================
quarkus.log.level=INFO
# Change this to set Spark log level
quarkus.log.category."org.apache.spark".level=WARN
# hadoop, parquet
quarkus.log.category."org.apache.hadoop".level=WARN
quarkus.log.category."org.apache.parquet".level=WARN
# Ignore messages below warning level from Jetty, because it's a bit verbose
quarkus.log.category."org.eclipse.jetty".level=WARN